{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import plotly\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss, mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More informations about this dataset here :https://vitaldb.net/dataset/?query=overview#h.1fo5zknztqnw\n",
    "df = pd.read_csv(\"operation_length_predictions_vitalDB.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4526, 25)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def calculate_duration_cols(df):\n",
    "    \"\"\"Calculate time durations for hospital, operation and anesthesia in hours from seconds\"\"\"\n",
    "    df['duration_stay'] = (df['dis'] - df['adm'])/3600\n",
    "    df['duration_operation'] = (df['opend'] - df['opstart'])/3600\n",
    "    df['duration_anesthesia'] = (df['aneend'] - df['anestart'])/3600\n",
    "    return df\n",
    "\n",
    "def drop_id_cols(df):\n",
    "    \"\"\"Drop ID columns and original timestamp columns\"\"\"\n",
    "    columns_to_drop = [\n",
    "        'caseid', 'subjectid',\n",
    "        'dis', 'adm',\n",
    "        'opend', 'opstart',\n",
    "        'aneend', 'anestart',\n",
    "        'casestart','caseend', \n",
    "        'icu_days',\"dx\",\"opname\",\n",
    "        'preop_ecg','preop_ph', 'preop_hco3', 'preop_be', 'preop_pao2', \n",
    "        'preop_paco2', 'preop_sao2', 'cormack', 'tubesize',\n",
    "        'dltubesize', 'lmasize', 'iv2', 'aline1', 'aline2',\n",
    "        'cline1', 'cline2',\"preop_na\",\"preop_k\",'airway','iv1','duration_stay',\n",
    "        'death_inhosp'\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=df.filter(like='intraop').columns)\n",
    "\n",
    "    return df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "def map_position(df):\n",
    "    position_mapping = {\n",
    "    'Supine': 'Supine',\n",
    "    'Lithotomy': 'Lithotomy',\n",
    "    'Left lateral decubitus': 'Lateral Decubitus',\n",
    "    'Right lateral decubitus': 'Lateral Decubitus',\n",
    "    'Prone': 'Prone',\n",
    "    'Reverse Trendelenburg': 'Trendelenburg (Inclined)',\n",
    "    'Trendelenburg': 'Trendelenburg (Inclined)',\n",
    "    'Sitting': 'Sitting',\n",
    "    'Left kidney': 'Lateral Decubitus',\n",
    "    'Right kidney': 'Lateral Decubitus'\n",
    "    }\n",
    "\n",
    "    df['position'] = df['position'].map(position_mapping)\n",
    "    return df\n",
    "\n",
    "def map_ane_type(df):\n",
    "    ane_mapping = {\n",
    "        'General': 'General Anesthesia',\n",
    "        'Spinal': 'Regional Anesthesia',\n",
    "        'Sedationalgesia': 'Regional Anesthesia'\n",
    "    }\n",
    "\n",
    "    df['ane_type'] = df['ane_type'].map(ane_mapping)\n",
    "    return df\n",
    "\n",
    "def handle_age_more_than_89(df):\n",
    "    df.loc[df['age'] == '>89', 'age'] = 92\n",
    "    return df\n",
    "\n",
    "def map_preop_pft(df):\n",
    "    pft_mapping = {\n",
    "    \"Normal\": \"Normal\",\n",
    "    \"Mild obstructive\": \"Mild/Moderate Obstructive\",\n",
    "    \"Moderate obstructive\": \"Mild/Moderate Obstructive\",\n",
    "    \"Borderline obstructive\": \"Mild/Moderate Obstructive\",\n",
    "    \"Severe obstructive\": \"Severe Obstructive\",\n",
    "    \"Mixed or pure obstructive\": \"Severe Obstructive\",\n",
    "    \"Mild restrictive\": \"Mild/Moderate Restrictive\",\n",
    "    \"Moderate restrictive\": \"Mild/Moderate Restrictive\",\n",
    "    \"Severe restrictive\": \"Severe Restrictive\",\n",
    "    }\n",
    "    df['preop_pft'] = df['preop_pft'].map(pft_mapping)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def drop_na_rows(df):\n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "def encode_scale(df):\n",
    "\n",
    "    # label_encoder = OrdinalEncoder()\n",
    "    scaler = StandardScaler()\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # categorical_features = []\n",
    "    numerical_features = ['age', 'weight','height', 'bmi', 'asa', 'preop_hb', 'preop_plt', 'preop_pt', 'preop_aptt', 'preop_gluc', 'preop_alb', 'preop_ast', 'preop_alt', 'preop_bun','preop_cr']\n",
    "    onehot_features = ['preop_pft','sex', 'department', 'optype', 'approach', 'ane_type', 'position']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('cat', label_encoder, categorical_features),\n",
    "        ('num', scaler, numerical_features),\n",
    "        ('onehot', onehot_encoder, onehot_features)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    # verbose_feature_names_out=False\n",
    "    )\n",
    "    df_processed = preprocessor.fit_transform(df)\n",
    "    return df_processed\n",
    "\n",
    "def split_dataset(df,target_column='duration_anesthesia'):\n",
    "    \"\"\"Split the dataset into train and test sets\"\"\"\n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=['duration_operation','duration_anesthesia'], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def handle_outliers(X_train,y_train,target_column):\n",
    "    \"\"\"Handle outliers in the training set\"\"\"\n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df = df.drop(df[df[target_column] > 10].index)\n",
    "    df = df.drop(df[df[target_column] < 0].index)\n",
    "\n",
    "    y_train = df[target_column]\n",
    "    X_train = df.drop(columns=[target_column], axis=1)\n",
    "    print(X_train.shape)\n",
    "    return X_train,y_train\n",
    "\n",
    "def process_data(df,target_column):\n",
    "    \"\"\"Main function to process the dataframe\"\"\"\n",
    "    df = calculate_duration_cols(df)\n",
    "    df = drop_id_cols(df)\n",
    "    df = drop_na_rows(df)\n",
    "    df = map_position(df)\n",
    "    df = map_ane_type(df)\n",
    "    df = handle_age_more_than_89(df)\n",
    "    df = map_preop_pft(df)\n",
    "    # print(df.columns)\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df,target_column)\n",
    "    X_train, y_train = handle_outliers(X_train,y_train,target_column)\n",
    "    X_train = encode_scale(X_train)\n",
    "    X_test = encode_scale(X_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = process_data(df, target_column='duration_operation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:34:54,771] A new study created in memory with name: no-name-deceb4eb-d87c-400e-bd82-745d2bd548ff\n",
      "Best trial: 0. Best value: 0.18862:  10%|█         | 1/10 [01:17<11:35, 77.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:36:11,992] Trial 0 finished with value: 0.18862045292599353 and parameters: {'learning_rate': 0.0823682974050483, 'n_estimators': 428, 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 14, 'subsample': 0.615516319094151}. Best is trial 0 with value: 0.18862045292599353.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.179536:  20%|██        | 2/10 [01:41<06:10, 46.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:36:36,622] Trial 1 finished with value: 0.17953565462037208 and parameters: {'learning_rate': 0.1687459998414928, 'n_estimators': 185, 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'subsample': 0.8917530900976749}. Best is trial 1 with value: 0.17953565462037208.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  30%|███       | 3/10 [02:24<05:12, 44.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:37:19,283] Trial 2 finished with value: 0.17492738627297155 and parameters: {'learning_rate': 0.0630733673326134, 'n_estimators': 265, 'max_depth': 4, 'min_samples_leaf': 10, 'min_samples_split': 6, 'subsample': 0.6777503426820692}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  40%|████      | 4/10 [02:50<03:44, 37.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:37:45,581] Trial 3 finished with value: 0.1780126587609124 and parameters: {'learning_rate': 0.24839466287575213, 'n_estimators': 309, 'max_depth': 2, 'min_samples_leaf': 10, 'min_samples_split': 12, 'subsample': 0.7177254830456813}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  50%|█████     | 5/10 [03:40<03:29, 41.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:38:35,333] Trial 4 finished with value: 0.20473121049923254 and parameters: {'learning_rate': 0.2891281477772789, 'n_estimators': 322, 'max_depth': 4, 'min_samples_leaf': 14, 'min_samples_split': 13, 'subsample': 0.7220878629770586}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  60%|██████    | 6/10 [04:38<03:09, 47.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:39:33,512] Trial 5 finished with value: 0.18145607553397408 and parameters: {'learning_rate': 0.14911185769544777, 'n_estimators': 430, 'max_depth': 3, 'min_samples_leaf': 12, 'min_samples_split': 3, 'subsample': 0.9004763057605552}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  70%|███████   | 7/10 [05:50<02:45, 55.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:40:45,029] Trial 6 finished with value: 0.1883781353497723 and parameters: {'learning_rate': 0.1024058354868207, 'n_estimators': 303, 'max_depth': 5, 'min_samples_leaf': 11, 'min_samples_split': 12, 'subsample': 0.8819898305499425}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  80%|████████  | 8/10 [06:11<01:28, 44.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:41:06,214] Trial 7 finished with value: 0.18495531349051617 and parameters: {'learning_rate': 0.020157808036741363, 'n_estimators': 193, 'max_depth': 2, 'min_samples_leaf': 14, 'min_samples_split': 11, 'subsample': 0.9109759717399742}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.174927:  90%|█████████ | 9/10 [07:33<00:56, 56.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:42:28,520] Trial 8 finished with value: 0.17736639541628485 and parameters: {'learning_rate': 0.031409907348925487, 'n_estimators': 412, 'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5, 'subsample': 0.6718053540612333}. Best is trial 2 with value: 0.17492738627297155.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.174355: 100%|██████████| 10/10 [07:59<00:00, 47.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 18:42:54,155] Trial 9 finished with value: 0.17435500774868676 and parameters: {'learning_rate': 0.2549625640188951, 'n_estimators': 260, 'max_depth': 2, 'min_samples_leaf': 3, 'min_samples_split': 13, 'subsample': 0.882623021280744}. Best is trial 9 with value: 0.17435500774868676.\n",
      "Best parameters: {'learning_rate': 0.2549625640188951, 'n_estimators': 260, 'max_depth': 2, 'min_samples_leaf': 3, 'min_samples_split': 13, 'subsample': 0.882623021280744}\n",
      "\n",
      "Model Evaluation:\n",
      "Pinball Loss: 0.1640\n",
      "Coverage (should be close to 0.95): 0.9339\n"
     ]
    }
   ],
   "source": [
    "def objective(trial, X, y, n_splits=4):\n",
    "    \"\"\"Optuna objective function with cross-validation for 0.95 quantile\"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "        y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train model for 95th percentile\n",
    "        model = GradientBoostingRegressor(\n",
    "            loss=\"quantile\",\n",
    "            alpha=0.95,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        # Fit and evaluate\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred = model.predict(X_fold_val)\n",
    "        \n",
    "        # Calculate pinball loss\n",
    "        score = mean_pinball_loss(y_fold_val, y_pred, alpha=0.95)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def optimize_and_train_model(X_train, y_train, n_trials=10):\n",
    "    \"\"\"Optimize hyperparameters and return the best model\"\"\"\n",
    "    # Create study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), \n",
    "                  n_trials=n_trials,\n",
    "                  show_progress_bar=True)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_model = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=0.95,\n",
    "        random_state=42,\n",
    "        **best_params\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "    \n",
    "    return final_model, study,best_params\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model on test data\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    pinball_loss = mean_pinball_loss(y_test, y_pred, alpha=0.95)\n",
    "    \n",
    "    # Calculate percentage of predictions above actual values\n",
    "    coverage = np.mean(y_test <= y_pred)\n",
    "    \n",
    "    return {\n",
    "        'pinball_loss': pinball_loss,\n",
    "        'coverage': coverage\n",
    "    }\n",
    "\n",
    "\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "best_model, study,best_params = optimize_and_train_model(\n",
    "    X_train_array, \n",
    "    y_train_array, \n",
    "    n_trials=10\n",
    ")\n",
    "\n",
    "metrics = evaluate_model(best_model, X_test, y_test)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Pinball Loss: {metrics['pinball_loss']:.4f}\")\n",
    "print(f\"Coverage (should be close to 0.95): {metrics['coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train  upper bound model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinball Losses: {'q 0.50': 0.41926749970344107, 'q 0.95': 0.17209318888750916}\n",
      "Coverage (95%): 0.9444444444444444\n",
      "MSE (Median Model): 1.6425067568037361\n",
      "MAE (Median Model): 0.8385349994068821\n"
     ]
    }
   ],
   "source": [
    "def train_quantile_models(X_train, y_train):\n",
    "    \"\"\"Train Gradient Boosting models for quantile regression with alpha=0.5 and alpha=0.95 (upper bound with 95% confidence)\"\"\"\n",
    "    all_models = {}\n",
    "    common_params = dict(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=200,\n",
    "        max_depth=2,\n",
    "        min_samples_leaf=9,\n",
    "        min_samples_split=9,\n",
    "    )\n",
    "\n",
    "    for alpha in [0.5, 0.95]:\n",
    "        model = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "        all_models[f\"q {alpha:.2f}\"] = model.fit(X_train, y_train)\n",
    "    return all_models\n",
    "\n",
    "def evaluate_models(all_models, X_test, y_test):\n",
    "    \"\"\"Compute evaluation metrics for quantile regression models\"\"\"\n",
    "    y_pred_dict = {q: model.predict(X_test) for q, model in all_models.items()}\n",
    "\n",
    "    y_pred_median = y_pred_dict[\"q 0.50\"]\n",
    "    y_pred_upper = y_pred_dict[\"q 0.95\"]\n",
    "\n",
    "    # Compute metrics\n",
    "    pinball_losses = {q: mean_pinball_loss(y_test, y_pred_dict[q], alpha=float(q.split()[1])) \n",
    "                      for q in y_pred_dict}\n",
    "\n",
    "    coverage = (y_test <= y_pred_upper).mean()  # Since we have only the upper bound\n",
    "    mse_median = mean_squared_error(y_test, y_pred_median)\n",
    "    mae_median = mean_absolute_error(y_test, y_pred_median)\n",
    "\n",
    "    results = {\n",
    "        \"Pinball Losses\": pinball_losses,\n",
    "        \"Coverage (95%)\": coverage,\n",
    "        \"MSE (Median Model)\": mse_median,\n",
    "        \"MAE (Median Model)\": mae_median\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "all_models = train_quantile_models(X_train, y_train)\n",
    "metrics = evaluate_models(all_models, X_test, y_test)\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_upper_quantile_model(X_full, y_full):\n",
    "    \"\"\"Train a single Gradient Boosting model for 0.95 quantile\"\"\"\n",
    "    model = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=0.95,\n",
    "        **best_params # best params identified during the tuning procedure\n",
    "    )\n",
    "    model.fit(X_full, y_full)\n",
    "    return model\n",
    "\n",
    "# Combine training and test data\n",
    "X_full = np.concatenate([X_train, X_test], axis=0)\n",
    "y_full = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# Train model\n",
    "upper_model = train_upper_quantile_model(X_full, y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantile_model_95.joblib']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "import joblib\n",
    "joblib.dump(upper_model, 'quantile_model_95.joblib')\n",
    "\n",
    "# To load and use the model later:\n",
    "# loaded_model = joblib.load('quantile_model_95.joblib')\n",
    "# predictions = loaded_model.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
