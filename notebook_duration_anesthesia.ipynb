{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss, mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More informations about this dataset here :https://vitaldb.net/dataset/?query=overview#h.1fo5zknztqnw\n",
    "df = pd.read_csv(\"datasets/operation_length_predictions_vitalDB.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_duration_cols(df):\n",
    "    \"\"\"Calculate time durations for hospital, operation and anesthesia in hours from seconds\"\"\"\n",
    "    df['duration_stay'] = (df['dis'] - df['adm'])/3600\n",
    "    df['duration_operation'] = (df['opend'] - df['opstart'])/3600\n",
    "    df['duration_anesthesia'] = (df['aneend'] - df['anestart'])/3600\n",
    "    return df\n",
    "\n",
    "def drop_id_cols(df):\n",
    "    \"\"\"Drop ID columns and original timestamp columns\"\"\"\n",
    "    columns_to_drop = [\n",
    "        'subjectid',\n",
    "        'dis', 'adm',\n",
    "        'opend', 'opstart',\n",
    "        'aneend', 'anestart',\n",
    "        'casestart','caseend', \n",
    "        'icu_days',\"dx\",\"opname\",\n",
    "        'preop_ecg','preop_ph', 'preop_hco3', 'preop_be', 'preop_pao2', \n",
    "        'preop_paco2', 'preop_sao2', 'cormack', 'tubesize',\n",
    "        'dltubesize', 'lmasize', 'iv2', 'aline1', 'aline2',\n",
    "        'cline1', 'cline2',\"preop_na\",\"preop_k\",'airway','iv1','duration_stay',\n",
    "        'death_inhosp'\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=df.filter(like='intraop').columns)\n",
    "\n",
    "    return df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "def map_position(df):\n",
    "    position_mapping = {\n",
    "    'Supine': 'Supine',\n",
    "    'Lithotomy': 'Lithotomy',\n",
    "    'Left lateral decubitus': 'Lateral Decubitus',\n",
    "    'Right lateral decubitus': 'Lateral Decubitus',\n",
    "    'Prone': 'Prone',\n",
    "    'Reverse Trendelenburg': 'Trendelenburg (Inclined)',\n",
    "    'Trendelenburg': 'Trendelenburg (Inclined)',\n",
    "    'Sitting': 'Sitting',\n",
    "    'Left kidney': 'Lateral Decubitus',\n",
    "    'Right kidney': 'Lateral Decubitus'\n",
    "    }\n",
    "\n",
    "    df['position'] = df['position'].map(position_mapping)\n",
    "    return df\n",
    "\n",
    "def map_ane_type(df):\n",
    "    ane_mapping = {\n",
    "        'General': 'General Anesthesia',\n",
    "        'Spinal': 'Regional Anesthesia',\n",
    "        'Sedationalgesia': 'Regional Anesthesia'\n",
    "    }\n",
    "\n",
    "    df['ane_type'] = df['ane_type'].map(ane_mapping)\n",
    "    return df\n",
    "\n",
    "def handle_age_more_than_89(df):\n",
    "    df.loc[df['age'] == '>89', 'age'] = 92\n",
    "    return df\n",
    "\n",
    "def map_preop_pft(df):\n",
    "    pft_mapping = {\n",
    "    \"Normal\": \"Normal\",\n",
    "    \"Mild obstructive\": \"Mild/Moderate Obstructive\",\n",
    "    \"Moderate obstructive\": \"Mild/Moderate Obstructive\",\n",
    "    \"Borderline obstructive\": \"Mild/Moderate Obstructive\",\n",
    "    \"Severe obstructive\": \"Severe Obstructive\",\n",
    "    \"Mixed or pure obstructive\": \"Severe Obstructive\",\n",
    "    \"Mild restrictive\": \"Mild/Moderate Restrictive\",\n",
    "    \"Moderate restrictive\": \"Mild/Moderate Restrictive\",\n",
    "    \"Severe restrictive\": \"Severe Restrictive\",\n",
    "    }\n",
    "    df['preop_pft'] = df['preop_pft'].map(pft_mapping)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def drop_na_rows(df):\n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "def encode_scale(df):\n",
    "    # label_encoder = OrdinalEncoder()\n",
    "    scaler = StandardScaler()\n",
    "    onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # categorical_features = []\n",
    "    numerical_features = ['age', 'weight','height', 'bmi', 'asa', 'preop_hb', 'preop_plt', 'preop_pt', 'preop_aptt', 'preop_gluc', 'preop_alb', 'preop_ast', 'preop_alt', 'preop_bun','preop_cr']\n",
    "    onehot_features = ['preop_pft','sex', 'department', 'optype', 'approach', 'ane_type', 'position']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('cat', label_encoder, categorical_features),\n",
    "        ('num', scaler, numerical_features),\n",
    "        ('onehot', onehot_encoder, onehot_features)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    # verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    save_path = 'models_and_preprocessing/preprocessor_duration_anesthesia.pkl'\n",
    "    df_processed = preprocessor.fit_transform(df)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(preprocessor, f)\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "def load_and_apply_transformers(df, save_path='models_and_preprocessing/preprocessor_duration_anesthesia.pkl'):\n",
    "    with open(save_path, 'rb') as f:\n",
    "        preprocessor = pickle.load(f)\n",
    "\n",
    "    df_processed = preprocessor.transform(df)\n",
    "    return df_processed\n",
    "\n",
    "def split_dataset(df,target_column='duration_anesthesia'):\n",
    "    \"\"\"Split the dataset into train and test sets\"\"\"\n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=['duration_operation','duration_anesthesia'], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def handle_outliers(X_train,y_train,target_column):\n",
    "    \"\"\"Handle outliers in the training set\"\"\"\n",
    "    df = pd.concat([X_train, y_train], axis=1)\n",
    "    df = df.drop(df[df[target_column] > 10].index)\n",
    "    df = df.drop(df[df[target_column] < 0].index)\n",
    "\n",
    "    y_train = df[target_column]\n",
    "    X_train = df.drop(columns=[target_column], axis=1)\n",
    "    # print(X_train.shape)\n",
    "    return X_train,y_train\n",
    "\n",
    "def process_data(df,target_column):\n",
    "    \"\"\"Main function to process the dataframe\"\"\"\n",
    "    df = calculate_duration_cols(df)\n",
    "    df = drop_id_cols(df)\n",
    "    df = drop_na_rows(df)\n",
    "    df = map_position(df)\n",
    "    df = map_ane_type(df)\n",
    "    df = handle_age_more_than_89(df)\n",
    "    df = map_preop_pft(df)\n",
    "\n",
    "    sample = df[df[\"caseid\"]<6]\n",
    "    df = df[df[\"caseid\"]>6].reset_index(drop=True)\n",
    "    df = df.drop(columns=['caseid'], axis=1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_dataset(df,target_column)\n",
    "    X_train, y_train = handle_outliers(X_train,y_train,target_column)\n",
    "    X_train = encode_scale(X_train)\n",
    "    X_test = load_and_apply_transformers(X_test)\n",
    "    return X_train, X_test, y_train, y_test,sample\n",
    "\n",
    "X_train, X_test, y_train, y_test,sample = process_data(df, target_column='duration_anesthesia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.to_csv(\"sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 03:21:04,847] A new study created in memory with name: no-name-adba6c6f-8a5f-4df8-8060-d5e005131e3f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709ba75fe14044d08005b7252606e371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-16 03:21:22,113] Trial 0 finished with value: 0.2761709670153281 and parameters: {'learning_rate': 0.11593253773852119, 'n_estimators': 372, 'max_depth': 2, 'min_samples_leaf': 14, 'min_samples_split': 10, 'subsample': 0.8948923707683191}. Best is trial 0 with value: 0.2761709670153281.\n",
      "[I 2025-02-16 03:21:45,211] Trial 1 finished with value: 0.2885702806878709 and parameters: {'learning_rate': 0.21278747078520102, 'n_estimators': 206, 'max_depth': 5, 'min_samples_leaf': 6, 'min_samples_split': 6, 'subsample': 0.9259242185523049}. Best is trial 0 with value: 0.2761709670153281.\n",
      "[I 2025-02-16 03:21:52,382] Trial 2 finished with value: 0.27600923123348897 and parameters: {'learning_rate': 0.06406078222241268, 'n_estimators': 144, 'max_depth': 3, 'min_samples_leaf': 8, 'min_samples_split': 13, 'subsample': 0.6256636900136544}. Best is trial 2 with value: 0.27600923123348897.\n",
      "[I 2025-02-16 03:22:28,271] Trial 3 finished with value: 0.27901865745960097 and parameters: {'learning_rate': 0.11555770272772702, 'n_estimators': 500, 'max_depth': 3, 'min_samples_leaf': 10, 'min_samples_split': 3, 'subsample': 0.9585540008489822}. Best is trial 2 with value: 0.27600923123348897.\n",
      "[I 2025-02-16 03:22:40,657] Trial 4 finished with value: 0.2770606356113378 and parameters: {'learning_rate': 0.21293725216364362, 'n_estimators': 261, 'max_depth': 2, 'min_samples_leaf': 13, 'min_samples_split': 9, 'subsample': 0.9526176841601814}. Best is trial 2 with value: 0.27600923123348897.\n",
      "[I 2025-02-16 03:23:42,560] Trial 5 finished with value: 0.3074002119781696 and parameters: {'learning_rate': 0.2066985511881889, 'n_estimators': 473, 'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 6, 'subsample': 0.766793701478844}. Best is trial 2 with value: 0.27600923123348897.\n",
      "[I 2025-02-16 03:24:10,793] Trial 6 finished with value: 0.2918693894118996 and parameters: {'learning_rate': 0.28399239183605135, 'n_estimators': 271, 'max_depth': 4, 'min_samples_leaf': 13, 'min_samples_split': 15, 'subsample': 0.8496692940862844}. Best is trial 2 with value: 0.27600923123348897.\n",
      "[I 2025-02-16 03:24:53,198] Trial 7 finished with value: 0.2807116983227944 and parameters: {'learning_rate': 0.13514428540847415, 'n_estimators': 401, 'max_depth': 5, 'min_samples_leaf': 15, 'min_samples_split': 3, 'subsample': 0.9815474005352671}. Best is trial 2 with value: 0.27600923123348897.\n",
      "[I 2025-02-16 03:25:05,061] Trial 8 finished with value: 0.27082357543124574 and parameters: {'learning_rate': 0.06055236856467801, 'n_estimators': 341, 'max_depth': 3, 'min_samples_leaf': 12, 'min_samples_split': 14, 'subsample': 0.8494026107044984}. Best is trial 8 with value: 0.27082357543124574.\n",
      "[I 2025-02-16 03:25:11,451] Trial 9 finished with value: 0.27710231491895204 and parameters: {'learning_rate': 0.22313240409931043, 'n_estimators': 242, 'max_depth': 2, 'min_samples_leaf': 3, 'min_samples_split': 5, 'subsample': 0.9399084920162607}. Best is trial 8 with value: 0.27082357543124574.\n",
      "Best parameters: {'learning_rate': 0.06055236856467801, 'n_estimators': 341, 'max_depth': 3, 'min_samples_leaf': 12, 'min_samples_split': 14, 'subsample': 0.8494026107044984}\n",
      "\n",
      "Model Evaluation:\n",
      "Pinball Loss: 0.3015\n",
      "Coverage (should be close to 0.90): 0.8641\n"
     ]
    }
   ],
   "source": [
    "def objective(trial, X, y, n_splits=4):\n",
    "    \"\"\"Optuna objective function with cross-validation for 0.90 quantile\"\"\"\n",
    "    # Define hyperparameter search space\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 6),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_fold_train, X_fold_val = X[train_idx], X[val_idx]\n",
    "        y_fold_train, y_fold_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train model for 90th percentile\n",
    "        model = GradientBoostingRegressor(\n",
    "            loss=\"quantile\",\n",
    "            alpha=0.90,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        # Fit and evaluate\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_pred = model.predict(X_fold_val)\n",
    "        \n",
    "        # Calculate pinball loss\n",
    "        score = mean_pinball_loss(y_fold_val, y_pred, alpha=0.90)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "def optimize_and_train_model(X_train, y_train, n_trials=10):\n",
    "    \"\"\"Optimize hyperparameters and return the best model\"\"\"\n",
    "    # Create study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train), \n",
    "                  n_trials=n_trials,\n",
    "                  show_progress_bar=True)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_model = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=0.90,\n",
    "        random_state=42,\n",
    "        **best_params\n",
    "    )\n",
    "    final_model.fit(X_train, y_train)\n",
    "    \n",
    "    return final_model, study,best_params\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the model on test data\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    pinball_loss = mean_pinball_loss(y_test, y_pred, alpha=0.90)\n",
    "    \n",
    "    # Calculate percentage of predictions above actual values\n",
    "    coverage = np.mean(y_test <= y_pred)\n",
    "    \n",
    "    return {\n",
    "        'pinball_loss': pinball_loss,\n",
    "        'coverage': coverage\n",
    "    }\n",
    "\n",
    "X_train_array = np.array(X_train)\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "best_model, study,best_params = optimize_and_train_model(\n",
    "    X_train_array, \n",
    "    y_train_array, \n",
    "    n_trials=10\n",
    ")\n",
    "\n",
    "metrics = evaluate_model(best_model, X_test, y_test)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Pinball Loss: {metrics['pinball_loss']:.4f}\")\n",
    "print(f\"Coverage (should be close to 0.90): {metrics['coverage']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_upper_quantile_model(X_full, y_full):\n",
    "    \"\"\"Train a single Gradient Boosting model for 0.90 quantile\"\"\"\n",
    "    model = GradientBoostingRegressor(\n",
    "        loss=\"quantile\",\n",
    "        alpha=0.9,\n",
    "        **best_params # best params identified during the tuning procedure\n",
    "    )\n",
    "    model.fit(X_full, y_full)\n",
    "    return model\n",
    "\n",
    "# Combine training and test data\n",
    "X_full = np.concatenate([X_train, X_test], axis=0)\n",
    "y_full = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "# Train model\n",
    "upper_model = train_upper_quantile_model(X_full, y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models_and_preprocessing/quantile_model_90_duration_anesthesia.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(upper_model, 'models_and_preprocessing/quantile_model_90_duration_anesthesia.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.166667\n",
       "1    4.433333\n",
       "2    1.333333\n",
       "3    5.833333\n",
       "4    6.500000\n",
       "Name: duration_anesthesia, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['duration_anesthesia']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
